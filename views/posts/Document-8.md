<!--title: 关于logistic算法-->
<!--date: 2016-05-28-->
<!--tags: Machine Learning-->
<!--abstract: 学习了一段时间的机器学习，感觉有梯度下降算法，logistic算法，还有神经网络的基础——单层感知机和线性神经网络有一些共同点，可以互相帮助理解。于是乎，记录下来，让自己在写的同时理解的更加透彻。-->

%metaEnd%

##关于logistic算法

>学习了一段时间的机器学习，感觉有梯度下降算法，logistic算法，还有神经网络的基础——单层感知机和线性神经网络有一些共同点，可以互相帮助理解。于是乎，记录下来，让自己在写的同时理解的更加透彻。

###共同的迭代形式

	
在Andrew Ng的机器学习[讲义notes1](http://cs229.stanford.edu/notes/cs229-notes1.pdf)中第16页开始就阐述了logistic算法的详细过程。让人觉得眼前一亮的是结果：

![](http://cezrh.img48.wal8.com/img48/544629_20160502104557/147107833483.png)

和随机梯度下降算法的迭代过程长得一模一样有没有，唯一不同的是在随机梯度下降算法中那个hypothese函数h是一个线性函数，是权重w和训练样本x的乘积。而在logistic函数中h是logistic函数。

无独有偶，这个迭代算法同样也是线性神经网络中的LMS学习算法，一模一样，连函数h也是一样的。[讲义notes1](http://cs229.stanford.edu/notes/cs229-notes1.pdf)中，在logistic算法后面紧接着介绍的感知机学习算法，也是一模一样的迭代公式。不一样的是函数h，这时候它被叫做了激活函数，准确的说是在权重和训练样本的乘积之上套了一个激活函数。

接下来，我们具体看一下logistic算法。

###logistic function 

在机器学习[讲义notes1](http://cs229.stanford.edu/notes/cs229-notes1.pdf)第16页给出了这个函数，如下：

![](http://cezrh.img48.wal8.com/img48/544629_20160502104557/147107833382.png)

我们可以看到，在大牛的讲义上说这个函数被叫做logistic函数或者sigmoid函数。一开始看到的时候是有一点不解的，明明这一部分是讲的logistic算法呀，提出一个叫logistic的函数很理所当然，可是这个sigmoid是个什么鬼。

直到现在自己学习神经网络的时候，在BP神经网络模型推导的时候提到了这个函数，不过它的名字变成了单极性Sigmoid函数。

这个函数有个很重要的推导，在logstic算法推导中起了很重要的作用，如下：

![](http://cezrh.img48.wal8.com/img48/544629_20160502104557/147107854916.png)

所以个人觉得这一点注意好了，logistic函数使用起来就和梯度下降函数没多少区别了，毕竟从本质上来说，logistic函数的迭代过程就是遵循随机梯度下降的规则。


(完)